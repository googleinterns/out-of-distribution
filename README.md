# Adversarial Defense For Computer Vision

In 2012, the computer vision landscape was forever changed when the AlexNet convolutional neural network (CNN) architecture won a decisive victory in the ImageNet classification competition. Since then, CNNs have overtaken hand-engineered feature extractors as the dominant paradigm for image classification, object detection, and semantic segmentation tasks. Over the next few years, the ResNet, Faster R-CNN, and RefineNet architectures rose to prominence as state-of-the-art solutions for these respective tasks. Today, CNNs are used in a wide range of applications, from self-driving cars to automated analysis of medical imagery.

Due to their increasing prevalence in mission-critical applications, CNNs must remain robust and reliable under challenging input conditions. Unfortunately, CNNs are notoriously susceptible to adversarial examples, which are images crafted to look normal to humans while tricking CNNs into producing incorrect predictions with high confidence. As a result, adversarial defense has become a popular research topic, aiming to discover CNN architectures that are more resilient to adversarial examples.

The vast majority of adversarial defense research so far focuses on improving classification models. There is a paucity of published literature dealing with detection and segmentation models. In our project, we aim to close this gap. We examine the Max-Mahalanobis Center (MMC) loss, a popular classification defense, and apply it to the Faster-RCNN and RefineNet models. Our white paper is [here](docs/White%20Paper.pdf).
